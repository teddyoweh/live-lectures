{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "question = \"what color is cat in the image?\"\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in state.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def convert_video_to_audio(video_path, audio_path):\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(audio_path)\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    " \n",
    "\n",
    "convert_video_to_audio(\"w.MOV\", \"state.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64\n",
    "\n",
    "def encode_image(frame):\n",
    "    # Convert the frame to JPEG format\n",
    "    _, buffer = cv2.imencode('.jpg', frame)\n",
    "    # Encode the frame as base64 string\n",
    "    return base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "def summarize_text():\n",
    "    video_path = 'video.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    images_array = []\n",
    "\n",
    "    for second in range(0, int(frame_count // fps), 5):  # Iterate every 5 seconds\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(second * fps))\n",
    "        \n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        # Encode the frame as base64 and append to the array\n",
    "        base64_image = encode_image(frame)\n",
    "        images_array.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    cap.release()\n",
    "    max_num = 11\n",
    "    frame_text = ''\n",
    "\n",
    "    for i in range(0, len(images_array), max_num):\n",
    "        content = [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Describe whatâ€™s likely going on in frames {i} to {i + max_num}.\"\n",
    "        }] + images_array[i:i + max_num]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-vision-preview\",\n",
    "            messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        frame_text += str(response.choices[0].message.content).strip('`')  \n",
    "        print(f\"{i} to {i+15} frames\")\n",
    "\n",
    "    return frame_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_path = \"./teddy.jpeg\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-vision-preview\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "\n",
    "        {\"type\": \"text\", \"text\": \"what gender are they.\"},\n",
    "        # {\"type\":\"text\", f\"text\": \"this is the transcript of the frames - {cssx}\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=1000,\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(response.choices[0].message.content.strip('`').strip('json').strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frames(video_path, frame_interval=5):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    frames = []\n",
    "\n",
    "    while success:\n",
    "        if count % (frame_interval * int(vidcap.get(cv2.CAP_PROP_FPS))) == 0:\n",
    "            frames.append(image)\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_grid(frames, grid_size):\n",
    "    grid = None\n",
    "    rows, cols = grid_size\n",
    "    frame_height, frame_width, _ = frames[0].shape\n",
    "\n",
    "    for r in range(rows):\n",
    "        row = frames[r * cols:(r + 1) * cols]\n",
    "        # Resize frames to ensure they have the same dimensions\n",
    "        row_resized = [cv2.resize(frame, (frame_width, frame_height)) for frame in row]\n",
    "        row_concatenated = cv2.hconcat(row_resized)\n",
    "        if grid is None:\n",
    "            grid = row_concatenated\n",
    "        else:\n",
    "            grid = cv2.vconcat([grid, row_concatenated])\n",
    "\n",
    "    return grid\n",
    "\n",
    "def save_grid(grid, output_path):\n",
    "    cv2.imwrite(output_path, grid)\n",
    "\n",
    "def generate_grids(video_path, output_folder, frame_interval=5, max_frames_per_grid=12):\n",
    "    frames = extract_frames(video_path, frame_interval)\n",
    "    num_frames = len(frames)\n",
    "    num_grids = (num_frames - 1) // max_frames_per_grid + 1\n",
    "\n",
    "    for i in range(num_grids):\n",
    "        start_idx = i * max_frames_per_grid\n",
    "        end_idx = min((i + 1) * max_frames_per_grid, num_frames)\n",
    "        grid_frames = frames[start_idx:end_idx]\n",
    "        grid_size = (min(len(grid_frames), 3), min(len(grid_frames), 4))\n",
    "        grid = create_grid(grid_frames, grid_size)\n",
    "        save_path = f\"{output_folder}/grid_{i + 1}.jpg\"\n",
    "        save_grid(grid, save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"./video.mp4\"\n",
    "    output_folder = \"./output/grids\"\n",
    "    frame_interval = 5  # 5 seconds\n",
    "    max_frames_per_grid = 12\n",
    "\n",
    "    generate_grids(video_path, output_folder, frame_interval, max_frames_per_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "okay hey so this platform is called retract especially we decide to create a new way for students to learn and a new way for students to stay on top of your studies and now I'll show you a demo so first of all with this platform you can have life transcriptions of your lectures you can have live stories of your lectures of what's going on in lecture like basically fine and lecture you know something to work on this next week you know silica to do list I know so you know cuz I said it's going to next week that can also be turned into a calendar like you can easily just like you button and I guess turned into an event on your calendar that's amazing Teddy first off I want to ask you do you know that tomorrow at April 7th at 6:00 p.m. the hackathon will be over and you have to submit your final presentation yeah yeah I know yeah I know I know okay a couple of questions that I have first off this is an amazing platform that you guys created I want to know what are some potential stakeholders that would be using this platform other than just students it's good that we found it now though what does the button change anyway back to what you were saying okay so yeah I used to stop I don't know I'll just copy this can you type in there cracked way before you wait it's just so let me add something to it so yeah this software is perfect for all types of people especially people who are in the workforce things like Consultants Bankers investment bankers and even teachers it can be applicable to anyone who needs to go into a meeting or is very lazy just so you can have a question I'm joking last night it's my house JoJo Joanna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt =\" So this our platform is called Retract, and Retraff is actually going to redefine the ways to this right. So what we decided to do, we have like multiple stuff. Our first of all is what we decide to do. So that's different, first of all, you know, a feature whereby students can, you know, have live transcript of live lectures. Let's say, for example, they're not enough, like with this platform, they can stay on track, like, what's you can see visibly, they also get like, they also get like the, the, the, the, the, the... like the summary of what's going on in the lecture real time they also get like an action action like key stuff as we mentioned and also doing it like a to do this on like let's say okay you have to do that and also this also serves as you know this is like a digital source that we're really used like you know creates very well structured the mission note and just like practice quizzes. in the hackathon I have a couple follow-up questions that if you would in mind I would like you to explain The first question is how did you guys get go about for the idea and what kind of codes did you use to create this? So personally I have a hard time to like constriction and class like because like it's just like there's just so much to do like this weird but\"\n",
    "staxx = str(\n",
    "  {'summary': \"The conversation introduces 'Retract', a platform designed to enhance students' learning and organizational skills. It features live transcriptions and summaries of lectures, along with a to-do list functionality that integrates with calendar events. The platform is also deemed valuable for professionals such as consultants and investment bankers. A demo was shown, emphasizing its practical applications. The discussion briefly touches on a hackathon deadline.\",\n",
    " 'action_items': ['Show demo of Retract platform',\n",
    "  'Work on integration with calendar',\n",
    "  'Prepare for hackathon submission'],\n",
    " 'todo': ['Integrate to-do list with calendar'],\n",
    " 'questions': {'Who are potential stakeholders for the platform?': 'Besides students, potential stakeholders include professionals like consultants, bankers, investment bankers, and even teachers. The platform is versatile, catering to anyone who attends meetings or needs an efficient way to organize their tasks and notes.'},\n",
    " 'reminders': {'description': 'Hackathon submission deadline',\n",
    "  'time': '6:00 PM',\n",
    "  'day': '7',\n",
    "  'year': '2023',\n",
    "  'month': 'April'}}\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that returns summary, actions items, todo, questions, reminders (such as date).\"},\n",
    "    {\"role\": \"user\", \"content\": f\"this is the transcript of the conversation -{text}\"},\n",
    "    {\"role\": \"system\", \"content\": \"Please summarize the conversation and provide action items, todo, questions, and reminders in this json format. {'summary: , action_items: , todo: , questions: , reminders: }\"},\n",
    "    {\"role\": \"user\", \"content\": \"the summary should be conscise and to the point, the action items should be an array of short key stuff mentioned, the todo should be only key details mentioned in the transcript of things to do stated by the speaker do not make up todos, the questions should only key questions that what asked in the transcript, the reminders should be clear and actionable, questions should be in hashmap, have the question and the answer to the question ensure to task inivatatin and not just answer the question, but say why & how, be creative and innovative the answers, make the reminders be in a hashmap, description,time,day,year,month.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"this prior and data of the converation leading up to present transcript - {staxx} keep the summary in arrays, of prior converstiaons and the current conversation\"},\n",
    "\n",
    "    {\"role\": \"system\", \"content\": \"only add the the overall hashmap, do not take anything out \"},\n",
    "\n",
    " \n",
    "  ],\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': [\"The conversation introduces 'Retract', a platform designed to enhance students' learning and organizational skills. It features live transcriptions and summaries of lectures, along with a to-do list functionality that integrates with calendar events. The platform is also deemed valuable for professionals such as consultants and investment bankers. A demo was shown, emphasizing its practical applications. The discussion briefly touches on a hackathon deadline.\",\n",
       "  \"In the follow-up discussion, a demonstration of 'Retract' was further elaborated, highlighting its capabilities such as live transcriptions of lectures, story summaries of lectures, and an innovative feature that converts tasks into calendar events. The conversation also sheds light on the platform's broader applicability beyond students, suggesting its relevance for various professionals. A reminder about the hackathon submission deadline is reiterated, and questions are raised regarding potential stakeholders outside the student demographic.\"],\n",
       " 'action_items': ['Show demo of Retract platform',\n",
       "  'Work on integration with calendar',\n",
       "  'Prepare for hackathon submission',\n",
       "  'Discuss broader applicability of Retract'],\n",
       " 'todo': ['Integrate to-do list with calendar',\n",
       "  'Demonstrate live transcription and story summary features'],\n",
       " 'questions': {'Who are potential stakeholders for the platform?': 'Besides students, potential stakeholders include professionals like consultants, bankers, investment bankers, and even teachers. The platform is versatile, catering to anyone who attends meetings or requires an organized mechanism for their tasks and notes.'},\n",
       " 'reminders': {'description': 'Hackathon submission deadline',\n",
       "  'time': '6:00 PM',\n",
       "  'day': '7',\n",
       "  'year': '2023',\n",
       "  'month': 'April'}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ass = eval(response.choices[0].message.content)\n",
    "ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The conversation introduces 'Retract', a platform designed to enhance students' learning and organizational skills. It features live transcriptions and summaries of lectures, along with a to-do list functionality that integrates with calendar events. The platform is also deemed valuable for professionals such as consultants and investment bankers. A demo was shown, emphasizing its practical applications. The discussion briefly touches on a hackathon deadline.\",\n",
       " \"In the follow-up discussion, a demonstration of 'Retract' was further elaborated, highlighting its capabilities such as live transcriptions of lectures, story summaries of lectures, and an innovative feature that converts tasks into calendar events. The conversation also sheds light on the platform's broader applicability beyond students, suggesting its relevance for various professionals. A reminder about the hackathon submission deadline is reiterated, and questions are raised regarding potential stakeholders outside the student demographic.\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ass['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(11606) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             # STARTAPI # \n",
      "\n",
      "        ==========================================================\n",
      "        | @author: teddyoweh                                     | \n",
      "        | @website: https://teddyoweh.net:                       | \n",
      "        | @github: https://github.com/teddyoweh                  | \n",
      "        | @twitter: https://twitter.com/tedddyoweh               | \n",
      "        | @mail: teddy@teddyoweh.net                             | \n",
      "        | @message: reach out for contributions or questions ;)! | \n",
      "        ==========================================================\n",
      "        \n",
      "\n",
      "\n",
      "-[#] Building \u001b[93mNone\u001b[0m FLASK API CodeBase\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/teddyoweh/Documents/GitHub/retrak/ml-microservice/start.py\", line 2, in <module>\n",
      "    startapi()\n",
      "  File \"/Users/teddyoweh/Library/Python/3.9/lib/python/site-packages/startapi/startapi.py\", line 46, in __init__\n",
      "    self._build('flaskboilerplate')\n",
      "  File \"/Users/teddyoweh/Library/Python/3.9/lib/python/site-packages/startapi/startapi.py\", line 73, in _build\n",
      "    os.chdir(self.apiname)\n",
      "TypeError: chdir: path should be string, bytes, os.PathLike or integer, not NoneType\n"
     ]
    }
   ],
   "source": [
    "!python3 start.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
