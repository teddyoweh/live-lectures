" Hi everyone, welcome back. Today, I think that these two lectures today are really exciting because they start to move beyond a lot of what we've talked about in the class so far, which is focusing a lot on really static data sets. field of how we can specifically marry two topics. The first topic being reinforcement learning, which has existed for many, many decades, together with a lot of the very recent advances in deep learning, which you've already started learning about as part of this course. Now, this marriage of these two fields is actually really fascinating to me particularly because, like I said, it moves away from this whole paradigm of, or really this whole paradigm that we've been exposed to. in the class thus far and that paradigm is really how we can build a deep learning model using some data set but that data set is typically fixed in our world right we collect we go out and go collect that data set we deploy it on our machine learning or deep learning algorithm and then we can evaluate on a brand new data set. But that is very different than the way things work in the real world. In the real world, you have your deep learning model actually deployed together with the data, together out into reality, exploring, interacting with its environment, and trying out a whole bunch of different actions and different things in that environment in order to be able to learn how to best perform any particular tasks that it may need to accomplish. And typically, we want to be able to do this without explicit human supervision, right. enforcement learning. You're going to try and learn through reinforcement, making mistakes in your world and then collecting data on those mistakes to learn how to improve. Now this is obviously a huge field in or a huge topic in the field of robotics and autonomy. You can think of self-driving cars and robot manipulation, but also very recently we've started seeing incredible advances of deep reinforcement learning specifically also on the side of gameplay and strategy making as well. So one really cool thing is that now you can even imagine right this combination of robotics together with gameplay, right? now training robots to play against us in the real world, and I'll just play this very short video on Starcraft and Deep Mind. Perfect information and is played in real time. It also requires long-term planning and the ability to choose what action to take from millions and millions of possibilities. I'm hoping for a 5-0 not to lose any games, but I think the realistic goal would be 4 and 1 in my favour. confident and Tila was quite nervous before. The room was much more tense this time. I really didn't know what to expect. He's been playing Starcraft, pretty much since he's five. I'm learning something. It's much better than I expected. I would consider myself a good player right, but I lost every single one of five games. We're way ahead of why. Right. So let's maybe start and take a step back, first of all, and think about how reinforcement learning fits into this whole paradigm of all the different topics that you've been exposed to in this class so far. So as a whole, I think that we've really covered two different types of learning in this course to date. Up until now, we've really started focusing in the beginning part of the lectures, learning, supervised learning is in this domain where we're given data in the form of X's, our inputs, and our labels Y. And our goal here is to learn a function or a neural network that can learn to predict y given our inputs X. So for example, if you consider this example of an apple, right, observing a bunch of apples, we want to detect, you know, in the future if we see a new image of an apple to detect that this is indeed Now the second class of learning approaches that we've discovered yesterday and yesterday's lecture was that of unsupervised learning and in these algorithms you have only access to the data there's no notion of labels right this is what we learned about yesterday in these types of algorithms you're not trying to predict a label but you're trying to uncover some of the underlying structure what we were calling basically these latent variables these hidden in your data. So for example in this apple example, right? Using unsupervised learning the analogous example would basically be to build a model that could understand and cluster certain parts of these images together and maybe it doesn't have to understand that necessarily this is an image of an apple but it needs to understand that you know this image of the red apple is similar it has the same latent features and same semantic meaning as this black and white outline outline sketch of the apple. Now in today's lecture we're going to talk about yet another type of learning algorithms right in reinforcement learning we're going to be only given data in the form of what are called state action pairs right now Now states are observations, right? This is what the agent, let's call it, the neural network is going to observe. It's what it sees. The actions are the behaviors that this agent takes in those particular states. So the goal of reinforcement learning is to build an agent that can learn how to maximize what are called rewards. This is the third component that is specific to reinforcement learning. time steps in the future. So again, in this apple example, we might now see that the agent doesn't necessarily learn that, okay, this is an apple, or it looks like these other apples. Now it has to learn to, let's say, eat the apple, take an action, eat that apple, because it has learned that eating that apple makes it live longer, or survive because it doesn't starve. So in today, right, right, like I said, we're going to be focusing on this third type of learning paradigm, paradigm which is reinforcement learning. And before we go any further, I just want to start by building up some very key terminology and like basically background for all of you, so that we're all on the same page when we start discussing some of the more complex components of today's lecture. So let's start by building up some of this terminology. The first main piece of terminology is that of an agent. An agent is a being, basically, that can take actions. For example, you can think of an agent as a machine, that is, let's say, an autonomous drone that is making a delivery, or, for example, in a game, it could be Super Mario that's navigating inside of your video game. The algorithm itself, it's important to remember that the algorithm is the agent, right? We're trying to build an agent that can do these tasks, and the algorithm is that agent. So in life, for example, all of you are agents in life. The environment is the other kind of contrary approach or the contrary perspective to the agent. The environment is simply the world where that agent lives and where it operates, right? And where it exists and it moves around it. The agent can send commands to that environment in the form of what are called actions. It can take actions in that environment, and let's call for notation purposes. possible set of all actions that it could take is let's say a set of capital A right now it should be noted that agents at any point in time could choose amongst this let's say list of possible actions but of course in some situations your action space does not necessarily need to be a finite space right maybe you could take actions just going right or left or straight, you may steer at any continuous degree. Observations is essentially how the environment responds back to the agent, right? The environment can tell the agent you know what it should be seeing based on those actions that it just took and it responds in the form of what is called a state. A state is simply a concrete and immediate situation that the agent finds itself in at that particular moment. Now it's important to remember that unlike other types of learning that we've covered in this course, reinforcement learning is a bit unique because it has one more component here in addition to these other components which is called the reward. Now the reward is a feedback by which we measure or we can try to measure the success of a particular agent in its environment. So for example, in a video game, when Mario grabs a coin, for example, he wins points, right? So from a given state, an agent can send out any form of actions to take some decisions, and those actions may or may not result in rewards being collected and accumulated over time. Now, it's also very important to remember that not all actions result in immediate rewards. You may take some actions that will result that will result in a reward. in a delayed fashion, maybe in a few times, steps down the future, or maybe in life, maybe years. You may take an action today that results in a reward many, some time from now. But essentially, all of these tried to effectively evaluate some way of measuring the success of a particular action that an agent takes. So for example, when we look at the total reward that an agent accumulates over the course of its lifetime, we can simply sum up all of the rewards that an agent gets after a certain time T. So this capital R of T is the sum of all rewards from that point on into the future into infinity. And that can be expanded to look exactly like this. It's reward at time T plus the reward at time T plus the reward at time T plus 1 plus T goes 2, and so forth. very useful for all of us to consider not only the sum of all of these rewards but instead what's called the discounted sum. So you can see here I've added this gamma factor in front of all of the rewards and that discounting factor is essentially multiplied by every future reward that the agent sees and is discovered by the agent and the reason that we want to do this is actually this dampening factor is designed to make future rewards essentially worth less than rewards that we might see at this instant right at this moment right now you can think of this as basically enforcing some kind of short-term a greediness in the algorithm right so for example if I offered you a reward of five dollars today or a reward of five dollars in ten years from now I think all of you would prefer that five dollars today simply because we have that same discounting factor applied to this processing right we have that factor that that five dollars is not worth as much to us if it's given to us ten years in the future and that's exactly how this is captured here as well mathematically. factor is like multiply like I said multiplied at every single future award exponentially and it's important to understand that also typically this discounting factor is you know between zero and one there are some exceptional cases where maybe you want some strange behaviors and have a discounting factor greater than one but in general that's not something we're going to be talking about today. Now finally it's very important in in reinforcement learning this special function called the Q function which ties in a lot of these different components that I've just shared with you altogether. Now let's look at what this Q function is. So we already covered this R of T function, right? R of T is the discounted sum of rewards from time T all the way into the future, time infinity. But remember that this R of T, right, it's discounted, number one. we're going to try and build a Q function that captures the maximum or the best action that we could take that will maximize this reward. So let me say that one more time in a different way. The Q function takes as input two different things. The first is the state that you're currently in, and the second is a possible action that you could execute in this particular state. So here, S of T is that state at time T, A of T. Here, S of T is that state at time T. A of T is that action that you may want to take at time T. And the Q function of these two pieces is going to denote or capture what the expected total return would be of that agent if it took that action in that particular state. Now one thing that I think maybe we should all be asking ourselves now is this seems like a really powerful function right if you had access to this type of a function this Q function I think you could actually perform a lot of tasks right off the bat. So if you wanted to, for example, understand how to what actions to take in a particular state and let's suppose I gave you this magical cue function, does anyone have any ideas of how you could transform that could be function to directly infer what action should be taken. Yep. Given a state you can look at in your possible action space and pick the one that gives you the highest Q values. Exactly. So that's exactly right. So just to repeat that one more time, the Q function tells us for any possible action, right? What is the expected reward for that action to be taken? So if we wanted to take a specific action, given in a specific state, Ultimately we need to figure out which action is the best action. The way we do that from a cue function is simply to pick the action that will maximize our future reward. And we can simply try out, number one, if we have a discrete action space, we can simply try out all possible actions, compute their cue value for every single possible action based on the state that we currently find ourselves in. And then we pick the action that is going to result in the highest cue value. If we have a continuous action space, maybe we do something a bit more intelligent, maybe following the gradients along this Q value curve and maximizing it as part of an optimization procedure. But generally, in this lecture, what I want to focus on is actually how we can obtain this cue function to start with, right? I kind of skipped a lot of steps in that last slide where I just said, let's suppose I give you this magical Q function, how can you determine what action to take. but in reality we're not given that Q function. We have to learn that Q function using deep learning and that's what today's lecture is going to be talking about primarily is first of all how can we construct and learn that Q function from data and then of course the final step is use that Q function to you know take some actions in the real world. And broadly speaking there are two classes of the first class is what's going to be called value learning and that's exactly this process that we've just talked value learning tries to estimate our cue function, right? So to find that cue given our state and our action and then use that cue function to you know optimize for the best action to take given a particular state that we find ourselves in. The second class of algorithms which we'll touch on right at the end of today's lecture is kind of a different framing of the same approach, but instead of first optimizing the Q function and finding the Q value and then using that Q function to optimize our actions, what if we just try to directly optimize our policy, which is what action to take based on a particular state that we find ourselves in? If we do that, if we can obtain this function, right, then we can directly sample from that policy distribution to obtain the optimal action. We'll talk more details about that later into the lecture. first let's cover this first class of approaches which is Q learning approaches and we'll build up that intuition and that knowledge onto the second part of policy learning. So maybe let's start by just digging a bit deeper into the Q function specifically just to start to understand you know how we could estimate this in the beginning. But first let me introduce this game, maybe some of you able to move left or right this paddle on the bottom left or right and the objective is to move it in a way that this ball that's coming down towards the bottom of the screen can be you know bounced off of your paddle reflected back up and essentially you want to break out right reflect that ball back up to the top of the to break out, reflect that ball back up to the top of the screen towards the rainbow portion and keep breaking off every time you hit a pixel on the top of the screen you break off that pixel. The objective of the game is to basically eliminate all of those rainbow pixels. So you want to keep hitting that ball against the top of the screen until you remove all the pixels. Now the Q function tells us the expected total return or the total reward that we can expect based on a given state and action pair that we may find ourselves in this game. Now the first point I want to make here is that sometimes even for us as humans to understand what the Q value should be is sometimes quite unintuitive. So here's one example. Let's say we find these two state action pairs, right. Here is a and B two different options that we can be presented with in this game. A the ball is coming straight down towards us, that's our state. Our action is to do nothing and simply reflect that ball back up vertically up. The second situation, the state is basically that the ball is coming slightly at an angle, we're not quite underneath it yet and we need to move towards it and actually hit that ball in a way that you know will make it and not miss it hopefully right so hopefully that ball doesn't pass below us then the game would be over can you imagine you know which of these two options might have a higher Q value value for the network which one would result in a rate of reward for the neural network or for the agent. So how many people believe A would result in a higher return? Okay. How about B? Okay. How about someone who picked B? Can you tell me why B? Why are an agency you actually doing something? Okay. Yeah. How about more? For A you only have like the maximum you can take off is like one because after you you reflect your automatic coming back down, but then be bounce around that there's more than a more than what I would just. Exactly. And actually there's a very interesting thing. So when I first saw this actually it's uh it was very unintuitive for me why A is actually working much worse than B but in general with this very conservative action of B, you're kind of exactly like you said, the two answers were implying is that A is a very conservative action. You're kind of only going up and down. It will achieve a good reward. It will solve the game, right? In fact, it solves the game exactly like this right here. You can see in general, this action is going to be quite conservative. It's just bouncing up, hitting one point at a time from the top and breaking off very slowly, very slowly the board that you can see, but in general you see the board that it's being broken off is towards the center of the center of the board. not much on the edges of the board. If you look at B now, with B you're kind of having agency, like one of the answers said. You're coming towards the ball, and what that implies is that you're sometimes going to actually hit the corner of your paddle and have a very extreme angle on your paddle and hit the sides of the board as well. And it turns out that the algorithm, the agent, can actually learn that hitting the side of the board, can have some kind of unexpected consequences that look like this. So here you see it trying to enact that policy. It's targeting the sides of the board, but once it reaches a breakout on the side of the board, it found this hack in the solution. We now is breaking off a ton of points. So that was a kind of a trick that this neural network learned, which was a way that it even moves away from the ball as it's coming down just so we could move back towards it, just to hit on the corner on the corner on the corner and execute on those corner parts of the board and break out a lot of pieces for free almost. So now that we can see that sometimes obtaining the Q function can be a little bit unintuitive, but the key point here is that if we have the Q function we can directly use it to determine what is the best action that we can take in any given state that we find ourselves in. So now the question naturally is how can we train a neural network that can indeed learn this cue function? So the type of the neural network here naturally because we have a function that takes as input two things, let's imagine our neural network will also take as input these two objects as well. One object is going to be the state of the board. You can think of this as simply the pixels that are on the screen describing that board. So it's an image of the board at a particular time. Maybe you want to even provide two some sense of temporal information and some past history as well, but all of that information can be combined together and provided to the network in the form of a state. And in addition to that, you may also want to provide it some actions as well, right? So in this case, the actions that a neural network could take in this game is to move to the right, to the left to stay still, right? those could be three different actions that could be provided and parameterized to the input of a neural network. The goal here is to you know estimate the single number output that measures what is the expected value or the expected Q value of this neural network at this particular state action pair. Now oftentimes what you'll see is that if you wanted to evaluate let's suppose a very large action space it's going to be very inefficient to try the approach on the left with a very large what it would mean is that you'd have to run your neural network forward many different times, one time for every single element of your action space. So what if instead you only provided it an input of your state and as output you gave it let's say all n different cue values, one cue value for every single possible action. That way you only need to run your neural network once for the given state that you're in, and then that neural network will tell you for all possible actions what's the maximum. and pick the action that has the Kies Kew value. Now, what would happen, right, so actually the question I want to pose here is really, you know, we want to train one of these two networks. Let's stick with the network on the right for simplicity, just since it's a much more efficient version of the network on the left. And the question is, you know, how do we actually train that network on the right? and specifically I want all of you to think about really the best case scenario just to start with case scenario just to start with. How an agent would perform ideally in a particular situation or what would happen, right, if an agent took all of the ideal actions at any given state. This would mean that essentially the target return, right, the predicted or the value that we're trying to predict, the target is going to always be maximized. And this can serve as essentially the ground truth to the agent. Now, for example, to do this, we want to formulate a loss function that's going to essentially represent our expected return if we're able to take all of the best actions, right? So for example, if we select an initial reward, plus selecting some action in our action space that maximizes our expected return, then for the next future state state we need to apply that discounting factor and recursively apply the same equation and that simply turns into our target right now we can ask basically what does our neural network predict right so that's our target and we recall from previous lectures if we have a target value in this case our Q value is a continuous variable we have also a predicted variable that is going to come as part of the output of every single one of these potential actions that could be taken we can define what's called a Q loss which is essentially just a very simple mean squared error loss between these two continuous variables. We minimize their distance over two over many many different iterations of flying our neural network in this environment and observing not only the actions but most importantly after the action is committed or executed we can see exactly the ground truth Expected return right so we have the ground truth labels to train and and supervised this model directly from the actions that were executed as part of random selection for example. Now let me just stop right there and maybe summarize the whole process one more time and maybe a bit different terminology just to give everyone kind of a different perspective on this same problem. So our deep neural network that we're trying to train looks like this, right? It takes us input a state, it's trying to output N different numbers. Those N different numbers correspond to the Q value associated to N different actions, one Q value per action. stay where we are. Right. So the next step from this we saw, if we have this Q value output, what we can do with it is we can make an action, or we can even, let me be more formal about it, we can develop what's called a policy function. A policy function is a function that given a state, it determines what is the best action. So that's different than the Q function, right? function tells us given a state what is the best or what is the value the return of every action that we could take the policy function tells us one step more than that given a state what is the best action right so it's a very end-to-end way of thinking about the agent's decision-making process based on what I see right now, what is the action that I should take? And we can determine that policy function directly from the Q function itself simply by maximizing and optimizing all of the different Q values for all of the different actions that we see here. So for example here we can see that given this state the Q function has the result of these three different values has a Q value of 20 if it goes to the left has a Q value of 3. If it stays in the same place and it has a Q value of zero it's going to basically die after this iteration if it moves to the right because you can see that the ball is coming to the left, if it moves to the right, the game, the game, the game, the if it moves to the right because you can see that the ball is coming to the left if it moves to the right the game is over right so it needs to move to the left in order to do that in order to continue the game and the Q value value reflects that. The optimal action here is simply going to be the maximum of these three Q values in this case it's going to be 20 and then the action is going to be the corresponding action that comes from that 20 which is moving left. action back to the environment in the form of the game to execute the next step, right? And as the agent moves through this environment, it's going to be responded with not only by new pixels that come from the game, but more importantly, some reward signal. Now it's very important to remember that the reward signals in Pong, or sorry, in Atari breakout, are very sparse, right? a few time steps for that ball to travel back up to the top of the screen. So usually your rewards will be quite delayed, maybe at least by several time steps, sometimes even more if you're bouncing off of the corners of the screen. Now one very popular or a very famous approach that showed this was presented by deep mind, Google Deep Mind several years ago, where they showed that you could train a Q-value network and you can see the input on the left hand coming from the screen all the way to the actions of a controller on the right-hand side and you could train this one network for a variety of different tasks all across the Atari breakout ecosystem of games. And for each of these tasks, the really fascinating thing that they showed was for this very simple algorithm which really relies on random choice of selection of actions and then learning from actions that don't do very well that you discourage them and trying to to actions that did perform well more frequently, very simple algorithm, but what they found was even with that type of algorithm, they were able to surpass human level performance on over half of the game. There were some games that you can see here were still below human level performance, but as we'll see, this was really such an exciting advance because of the simplicity of the algorithm, and how, you know, clean the formulation of the training was. network for it to be able to learn how to play these games. You never had to teach any of the rules of the game. You only had to let it explore its environment play the game many many times against itself and learn directly from that data. Now there are several very important downsides of Q-learning and hopefully these are going to motivate the second part of today's lecture which we'll talk about. But the first one that I want to really convey to everyone here is that to discrete action spaces, right? Because you can think of this output space that we're providing is kind of like one number per action that could be taken. Now if we have a continuous action space, we have to think about clever ways to work around that. In fact, there are now more recently, there are some solutions to achieve Q-learning and continuous action spaces. But for the most part, Q learning is very well suited for discrete action spaces, and we'll talk about ways of overcoming that with other approaches a bit later. The policy that we're that we're learning, right? The Q function is giving rise to that policy, which is the thing that we're actually using to determine what action to take given any state. That policy is determined by, you know, deterministically optimizing that Q function. We simply look at the results from the Q function and apply our, or we look at the results of the Q function, and we pick the action that has the best or the highest Q value. That is very dangerous in many cases because of the fact that it's always going to pick the best value for a given state. There's no stochasticity in that pipeline. So you can very frequently get caught in situations where you keep repeating the same actions, and you don't learn to explore potentially different options that you may be thinking of. So to address these very important challenges, of today's lecture which is going to be focused on policy learning, which is a different class of reinforcement learning algorithms that are different than Q-learning algorithms. And like I said, those are called policy gradient algorithms. And policy gradient algorithms, the main difference is that instead of trying to infer the policy from the policy from the policy, right? So it kind of skips one step and we'll see how we can train those networks. So before we get there let me just revisit one more time the Q function illustration that we are looking at right Q function we are trying to build a neural network outputs these Q values one value per action and we determine the policy by looking over this state of Q values picking the value that has the highest and and looking at its corresponding action. Now with policy networks, the idea that we want to keep here is that instead of predicting the Q values themselves, let's directly try to optimize this policy function. Here we're calling the policy function Pye of S, right? So Pye is the policy, S is our state, so it's a function that takes as input only the state, and it's going to directly output the action. So the outputs here give us the desired action that we should take in any give us in any give us the desired action that we should take in any given state that we find ourselves in. And that represents not only the best action that we should take, but let's denote this as basically the probability that selecting that action would result in a very desirable outcome for our network. So not necessarily the value of that action, but rather the probability that selecting that action would be the highest value, right? So you don't care exactly about what is the numerical value that selecting this action takes, or it gives rise to rather, but rather what is the likelihood that selecting this action will give you the best performing value that you could expect. The exact value itself doesn't matter. You only care about if selecting this action is going to give you with high likelihood the best one. So we can see that if these predicted probabilities here, right, in this example of Atari, right, going left has the probability of being the highest value action with 90% staying in the center, that's a probability. staying in the center that's a probability of 10% going right is 0%. So ideally what our neural network should do in this case is 90% of the time in this situation go to the left 10% of the time it could still try staying it where it is but never it should go to the right. Now note that this now is a probability distributions is very different than a Q function. A Q function has actually no structure, right? The Q values themselves can take any real number, right? But here, the policy network has a very formulated output. All of the numbers here in the output have to sum to one because this is a probability distribution, right? And that gives it a very rigorous version of how we can train this model that makes it a bit easier to train than a as well. So one other very important advantage of having an output that is a probability distribution is actually going to tie back to this other issue of Q functions and Q neural networks that we saw before and that is the fact that Q functions are naturally suited towards discrete action spaces. Now when we're looking at this policy network, we're outputing a distribution, right? And remember those distributions can also take continuous forms. In fact, we've seen this in the last two lectures, right? In the generative lecture, we saw how VAEs could be used to predict Gaussian distributions over their latent space. In the last lecture, we also saw how we could learn to predict uncertainties which are continuous probability distributions using data. And just like that, we could also use this same formulation to move beyond discrete action like you can see here, which are one possible action, a probability associated to one possible action, in a discrete set of possible actions. Now we may have a space which is not what action should I take, go left, right, or stand in center, but rather how quickly should should I move and in what direction should I move right that is a continuous variable as opposed to a discrete variable and you could say that now the answer should look like this right moving very fast to the right versus very slow to the excuse me very fast to the left versus very slow to the left has this continuous spectrum that we may want to model. Now when we plot this entire distribution of taking an action giving a state you can see basically a very simple illustration of that right here. This this distribution has most of its mass over, or sorry it has all of its mass over the entire real number line first of all. It has most of its mass right in the optimal action space that we want to take. So if we want to determine the best action to take we would simply take the of this distribution, the highest point. That would be the speed at which we should move and the direction that we should move in. If we wanted to also try out different things and explore our space, we could sample from this distribution and still obtain some stochasticity. Now, let's look at an example of how we can actually model these continuous distributions. And actually we've already seen some examples of this in the previous two lectures, like I mentioned, but let's take a look specifically in the context of reinforcement learning and policy gradient learning. So instead of predicting this probability of taking an action, giving all possible states, which in this case there is now an infinite number of, because we're in the continuous domain, we can't simply predict a single probability for every possible action because there is an infinite number of them. So instead, what if we parameterized our action space by a distribution, right. So let's take, for example, for example, for example, what a distribution, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right, right. the Gaussian distribution. To parameterize a Gaussian distribution, we only need two outputs, right? We need a mean and a variance. Given a mean and a variance, we can actually have a probability mass and we can compute a probability over any possible action that we may want to take, just from those two numbers. So for example in this image here we may want to output a Gaussian that looks like this, right? its mean is centered at, let's see, negative 0.8, indicating that we should move basically left with a speed of 0.8 meters per second, for example. And again, we can see that because this is a probability distribution, because this is a probability distribution, because of the format of policy networks, right, we're enforcing that this is a probability distribution. That means that the integral now of this output, right, right, integral now of this of this outputs right by definition of it being a Gaussian must also integrate to one. Okay great so now let's maybe take a look at how policy gradient networks can be trained and you know step through that process as well as we look at a very concrete example and maybe let's start by just revisiting this reinforcement learning loop Now let's specifically consider the example of training an autonomous vehicle since I think that this is a particularly very intuitive example that we can walk through. The agent here is the vehicle, right? The state could be obtained through many sensors that could be mounted on the vehicle itself. So for example, autonomous vehicles are typically equipped with sensors like cameras, liddars, radars, etc. all of these are giving observational inputs to the vehicle. The action that we could take is a steering wheel angle. This is not a discrete variable. This is a continuous variable. It's actually an angle that could take any real number. And finally, the reward in this very simplistic example is the distance that we travel before we crash. OK, so now let's take a look at how we could train a policy gradient neural network to solve this task of self-driving cars as a concrete example. Remember that we have no training data, right? So we have to think about actually reinforcement learning is almost like a data acquisition plus learning pipeline combined together. So the first part of that data acquisition pipeline is first to initialize our agent, to go out and collect some data. So we start our vehicle, our agent, and in the beginning, of course, it knows nothing about driving. It's never been exposed to any of these rules of the environment or the observation before. policy, which right now is untrained entirely, until it terminates, right? Until it goes outside of some bounds that we define, we measure basically the reward as the distance that it traveled before it terminated, and we record all of the states, all of the actions, and the final reward that it obtained, until that termination, right? This becomes our mini data set that we'll use for the first round of training. Let's take those data sets and now will do one step of training. The first step of training that we'll do is to take, excuse me, to take the later half of our trajectory that our agent ran and decrease the probability of actions that resulted in low rewards. Now because the vehicle, we know the vehicle terminated, we can assume that all of the actions that occurred in the later half of this trajectory were probably not very good actions because they came very close to termination. So let's decrease the probability of all of those things happening again in the future. And we'll take all of the things that happened in the beginning half of our training episode, and we will increase their probabilities. Now again there's no reason why there shouldn't necessarily be a good action that we took in the first half of this trajectory and a bad action in the later half. But it's simply because actions that are in the later half were closer to a failure and closer determination that we can assume, for example, that these were probably suboptimal actions. But it's very possible that these are noisy rewards as well because it's such a sparse signal. It's very possible that you had some good actions at the end. trying to recover your car but you were just too late. Now repeat this process again. Re-initialize the agent one more time and run it until completion. Now the agent goes a bit farther, right, because you've decreased the probabilities at the end, increase the probabilities of the future, and you keep repeating this over and over again until you notice that the agent learns to perform better and better every time, until it finally converges. And at the end, the agent is able to basically follow follow follow follow. all the lanes, usually swerving a bit side to side while it does that, without crashing. And this is actually really fascinating because this is a self-driving car that we never taught anything about what a lane marker means, or what are the rules of the road, anything about that. This was a car that learned entirely just by going out, crashing a lot and trying to figure out what to do to not keep doing that in the future. Right. and the remaining question is actually how we can update. you know that policy as part of this algorithm that I'm showing you on the right and the left hand side right like how can we basically formulate that same algorithm and specifically the update equation steps four and five right here these are the two really important steps right of how we can use those two steps to train our policy and decrease the probability of bad events while promoting these likelihoods of all these good events loss function for a policy gradient neural network looks like this and then we'll start by dissecting it to understand why this works the way it does. So here we can see that the loss consists of two terms. The first term is this term in green, which is called the log likelihood of selecting a particular action. The second term is something that all of you are very familiar with already. This is simply the return at a specific time, right? So that's the expected return on rewards that you would get after this time point. Now let's assume that we got a lot of reward for a particular action that had a high log probability or a high probability. If we got a lot of reward for a particular action that had high probability, that means that we want to increase that probability even further. So we do it even more, or even more likelihood we sample that action again into the future. if we've selected, or let's say if we obtained a reward that was very low for an action that had high likelihood, we want the inverse effect, right? We never want to sample that action again in the future because it resulted in a low reward, right, and you'll notice that this loss function right here by including this negative, we're going to minimize the likelihood of achieving any action that had low rewards in this trajectory. on the car example, all the things that had low rewards were exactly those actions that came closest to the termination part of the vehicle. All the things that had high rewards were the things that came in the beginning. That's just the assumption that we make when defining our reward structure. Now we can plug this into the loss of gradient descent algorithm to train our neural network when we see this policy gradient algorithm, which you can see highlighted here, this policy gradient algorithm, which you can see highlighted here, this policy gradient algorithm, which you can see highlighted gradient is exactly of the policy part of the neural network. That's the probability of selecting an action given a specific state. And if you remember before when we defined, you know, what does it mean to be a policy function? That's exactly what it means, right? Given a particular state that you find yourself in, what is the probability of selecting a particular action with the highest likelihood? And that's, you know, exactly where this method gets its name from this policy gradient piece Now I want to take maybe just a very brief second towards the end of the class here just to talk about some of the challenges and keeping in line with the first lecture today some of the challenges of deploying these types of algorithms in the context of the real world, right? When you look at this training algorithm, what do you think are the shortcomings of this training algorithm and which step I guess specifically if we wanted to deploy this... approach into reality. Yeah, exactly. So it's step two, right? If you wanted to do this in reality, right, that essentially means that you want to go out, collect your car, crashing it a bunch of times just to learn how to not crash it, right? And that's, you know, that's simply not feasible, right, number one. It's also, you know, very dangerous, number two.. So there are ways around this, right? The number one way around this is that people try to train these types of models in simulation, right? Simulation is very safe because we're not going to actually be damaging anything real. It's still very inefficient because we have to run these algorithms a bunch of times, and crash them a bunch of times just learn not to crash. But at least now, at least from a safety point of view, it's much safer. for reinforcement learning and generally, very broadly speaking, modern simulators for vision specifically do not at all capture reality very accurately. In fact, there's a very famous notion called the sim to real gap, which is a gap that exists when you train algorithms in simulation and they don't extend to a lot of the phenomena that we see and the patterns that we see in reality. want them to just highlight here is that when we're training reinforcement learning algorithms we ultimately want them to be you know not operating in simulation we want them to be in reality and as part of our lab here at MIT we've been developing this very very cool brand new photorealistic simulation engine that goes beyond basically the paradigm of how simulators work today which is basically defining a model of their environment and trying to synthesize that model. Essentially these simulators are like glorified game engines, right? They all look very game-like when you look at them. But one thing that we've done is taking a data-driven approach. Using real data of the real world, can we build up synthetic environments that are super photorealistic and look like this? So this is a cool result that we created here at MIT developing this photorealistic simulation engine. This is actually an autonomous agent, not a real car, driving through our virtual simulator in a bunch of different types of different scenarios. So this simulator is called this that allows us to basically use real data that we do collect in the real world, but then re- simulate those same real roads. For example, let's say you take your car, you drive out on Mass Ave, you collect data of mass-ab, you collect data of a virtual agent into that same simulated environment observing new viewpoints of what that scene might have looked like from different types of perturbations or types of angles that it might be exposed to and that allows us to train these agents now entirely using reinforcement learning no human labels but importantly allow them to be transferred into reality because there's no sim to real gap anymore. we placed agents into our simulator we trained them using the exact algorithms that you learned about in today's lecture these policy gradient algorithms and all of the training was done entirely in simulation. Then we took these policies and we deployed them on board our full-scale autonomous vehicle. This is now in the real world, no longer in simulation and on the left hand side you can see basically this car driving through this environment completely autonomous in the real world. No transfer learning is done here. There is no augmentation of data from real world data. This is entirely trained using simulation and this represented actually the first time ever that reinforcement learning was used to train a policy end-to-end for an autonomous vehicle that could be deployed in reality. So that was something really cool that we created here at MIT. But now that we covered all of this foundations of reinforcement learning and policy learning, I want to touch on some other maybe very exciting applications that we're seeing. And one very popular application that a lot of people will tell you about and talk about is the game of go. So here, reinforcement learning agents could be actually tried to put against the test against you know grand master level go players and you know at the time achieved incredibly impressive results so for those of you who are not familiar with the game of go the game of go is played on a 19 by 19 board the rough objective of go is to claim basically more board pieces than your opponent, right? And through the grid of, sorry, through the grid that you can see here, this 19 by 19 grid, and while the game itself, the logical rules are actually quite simple, the number of possible action spaces and possible states that this board could be placed into is greater than the number of atoms in the universe. So this game, even though the rules are very simple in their logical definitions, is an extraordinarily complex game for an artificial algorithm to try and master. master. So the objective here was to build a reinforcement learning algorithm to master the game of Go, not only beating these gold standard softwares, but also what was at the time, like an amazing result was to beat the grand master level players. So the number one player in the world of Go was a human champion, obviously. So Google Deep Mind rose to this challenge. They created a couple years ago developing this solution, which is very much based in the exact same algorithms that you learned about in today's lecture, combining both the value part of this network with residual layers, which we'll cover in the next lecture tomorrow. And using a reinforcement learning pipeline, they were able to defeat the grand champion human players. And the idea of its core was actually very simple. The first step is that you train a neural network to basically watch human level experts. So this is not using reinforcement learning, using supervised learning, using the techniques that we covered in lectures one, two, and from this first step, the goal is to build like a policy that would imitate some of the rough patterns that a human type of player or a human grandmaster would take based on a given board state, the type of actions that they might execute. But then given this pre-trained model essentially, you could use it to bootstrap a reinforcement learning algorithm that would play against itself, in order to learn how to improve even beyond the human levels. So it would take its human understandings, try to imitate the humans, first of all, but then from that imitation, they would pin these two neural networks against themselves, play a game against themselves, and the winners would be receiving a reward. The losers would try to negate all of the actions that they may have acquired from their human counterparts, achieving superhuman performance. And one of the very important auxiliary tricks that brought this idea to be possible was the usage of this second network, this auxiliary network, which took as input the state of the board and tried to predict, you know, what are all of the different possible board states that might emerge from this particular state and what would their values be? What would their potential returns and their outcomes be? So this network was an auxiliary network that was almost hallucinating, different board states that it could take from this particular state and using those predicted values to guide its planning of, you know, what action should it take into the future. And finally, very much more recently, they extended, they extended this algorithm, they had, that they could not even use the human grandmasters in the beginning to imitate from in the beginning and bootstrap these algorithms. What if they just started entirely from scratch and just had two neural networks, never trained before, they started pinning themselves against each other, and you could actually see that you could without any human supervision at all, have a neural network, learn to not only outperform the solution that, or outperformed the humans, but also outperformed the bootstrapped by humans as well. So with that all summarized very quickly, what we've learned today and conclude for the day. So we've talked a lot about really the foundational algorithms underlying reinforcement learning. We saw two different types of reinforcement learning of how we could optimize these solutions. First being Q learning, where we're trying to actually estimate, given a state, what is the value that we might expect for any possible action? and the second way was to take a much more end-to-end approach and say how given a state that we see ourselves in what is the likelihood that I should take any given action to maximize the potential that I have in this particular state. And I hope that all of this was very exciting to you. Today we have a very exciting lab and kick off for the competition. And the deadline for these competitions will be, well it was originally set to be Thursday, which is tomorrow at 11 p. Thank you."